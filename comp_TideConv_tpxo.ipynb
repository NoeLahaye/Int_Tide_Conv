{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# comp_TideConv_tpxo\n",
    "compute barotropic to baroclinic tide conversion using [St. Laurent & Garrett 2002], adapted from C. Vic code.\n",
    "\n",
    "### This code:\n",
    "* regional scale: grid is regular and cartesioen (approximately)\n",
    "* stratification is horizontally homogeneous\n",
    "* tides are taken from TPXO8 or forcing file of ROMS computation\n",
    "* topography, longitude and latitude are retrieved from ROMS grid file\n",
    "\n",
    "### Limitations:\n",
    "Originally, [St. Laurent & Garrett 2002] is relevant for \"weak\" topography and no important variations of tide and bathymetry. Here, we implicetely assume that tidal constituents are \"frozen\" at every points. Besides, finite-depth effect are taken into account by using a finite lower bound of integration in the spectral space (roughly corresponding to mode 1), which is taken constant here (whereas variations of kh_1 scales like 1/H)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib     \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['text.usetex'] = False\n",
    "import scipy.interpolate as interp\n",
    "import scipy.signal as sig\n",
    "import scipy.stats as stats\n",
    "from netCDF4 import Dataset\n",
    "from SW_Density import SW_Density as rhop # temporary\n",
    "#from comp_rho import rhop\n",
    "from distance_sphere_matproof import dist_sphere_matproof\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "from change_coord import reproject_image_into_polar\n",
    "#from mpi4py import MPI \n",
    "from pad_coords import pad_coords   # for padding fields outside of domain\n",
    "from detrend_2d import detrend_2d\n",
    "clock  = datetime.now()\n",
    "\n",
    "doverb = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpi size is 1 vs. 16x16\n"
     ]
    }
   ],
   "source": [
    "# --- MPI parameters --- \n",
    "npx,npy = 16,16 # number of processors in x and y directions  \n",
    "#npx,npy = 1,1 # number of processors in x and y directions  \n",
    "#comm = MPI.COMM_WORLD\n",
    "#rank = comm.Get_rank()\n",
    "#size = comm.Get_size()\n",
    "rank, size = 0, 1\n",
    "if size!=npx*npy:\n",
    "    print('mpi size is {0} vs. {1}x{2}'.format(size,npx,npy))\n",
    "    #raise ValueError('number of subdomains different from number of procs -> failure!')\n",
    "\n",
    "# --- data location ---\n",
    "path_data = '/data0/project/vortex/lahaye/Tide_Conv/input_data/' # './input_data/' #\n",
    "path_data = '/net/krypton'+path_data     # if not on LOPS hub\n",
    "\n",
    "# --- climato ---\n",
    "clim = \"lucky\"\n",
    "if clim == \"lucky\":\n",
    "    cname = path_data+\"lucky_ts_meanSODA_winter.nc\"\n",
    "\n",
    "# --- topography dataset --- \n",
    "topo = \"lucky\"\n",
    "\n",
    "# --- tide dataset --- \n",
    "tide = 'tpxo8'\n",
    "if tide == 'lucky':\n",
    "    collot = True    # tides and topo defined on same points\n",
    "    uname = path_data+'luckym2_frc.nc' # see read_write_TPXO8_netcdf.py\n",
    "elif tide == 'tpxo8':\n",
    "    collot = False\n",
    "    uname = path_data+'TPXO8/uv.m2_tpxo8_atlas_30c_v1.nc'\n",
    "    hname = path_data+'TPXO8/grid_tpxo8_atlas_30c_v1.nc'\n",
    "    \n",
    "# --- global grid ---   \n",
    "colloc = True    # use same grid as topo // False not implemented\n",
    "if colloc:\n",
    "    nstep = 10\n",
    "else:\n",
    "    dspace = 1e3       # resolution of the final map in meter\n",
    "#lonmin_g,lonmax_g = -33,-32     # test box // None takes the whole domain defined by h  \n",
    "#latmin_g,latmax_g = 37,37.7  \n",
    "lonmin_g, latmin_g, lonmax_g, latmax_g = [None]*4  # entire domain\n",
    "\n",
    "# --- output location and name --- \n",
    "path_write = path_data.replace('input','output')\n",
    "file_write = 'Ef_{0}_{1}_mpi.{2:02d}.nc'.format(topo,tide,rank)   \n",
    "\n",
    "# --- miscellaneous --- \n",
    "if topo == \"lucky\":\n",
    "    file_topo = path_data+\"lucky_grd.nc\"\n",
    "    Lchk = 150e3    # length of window for computing topo spectrum\n",
    "    varx, vary = 'lon_rho', 'lat_rho'    # variable names in netCDF files\n",
    "    varh = 'h'\n",
    "\n",
    "zmin       = -100              # min depth to compute Ef [m], below contin. shelf roughly\n",
    "g          = 9.81              # gravity [m s-2]\n",
    "omega      = 7.2921e-5         # Earth's rotation rate [rad s-1]\n",
    "M2         = 2.*np.pi/(44700.) # M2 tide frequency [rad s-1] \n",
    "lonm,latm  = -32.16,37.17      # Momar mooring location \n",
    "Erad = 6371e3                  # Earth radius [m]\n",
    "\n",
    "### warning: make sure the following lines are in agreement with subsequent parameters and grids\n",
    "nxout = 256    # number of points for k modulus\n",
    "nxoth = 128    # number of points for k angle\n",
    "khout = np.linspace(1./Lchk,1./750/np.sqrt(2.),nxout)*np.pi  # k adim for output = k*bar(N)*H/sqrt(M2^2-f^2)\n",
    "thout = np.linspace(0,np.pi*2,nxoth+1)[:-1]  # theta for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ get mean topo on the grid, will be used to get N2b --------\n",
    "# load the entire grid (regional modelling purpose)\n",
    "nc   = Dataset(file_topo,'r') # etopo2 and srtm30 files have the same structure\n",
    "if topo == 'lucky':\n",
    "    lon_h = nc.variables[varx][:].T\n",
    "    lat_h = nc.variables[vary][:].T\n",
    "    nlon_h, nlat_h = lon_h.shape\n",
    "    if lonmin_g is None:\n",
    "        ix = [0]\n",
    "    else:\n",
    "        ix = [np.abs(lon_h-lonmin_g).argmin(axis=0).min()]\n",
    "    if lonmax_g is None:\n",
    "        ix.append(nlon_h)\n",
    "    else:\n",
    "        ix.append(np.abs(lon_h-lonmax_g).argmin(axis=0).max())\n",
    "    if latmin_g is None:\n",
    "        jy = [0]\n",
    "    else:\n",
    "        jy = [np.abs(lat_h-latmin_g).argmin(axis=1).min()]\n",
    "    if latmax_g is None:\n",
    "        jy.append(nlat_h)\n",
    "    else:\n",
    "        jy.append(np.abs(lat_h-latmax_g).argmin(axis=1).max())\n",
    "    h_t = -nc.variables[varh][jy[0]:jy[1],ix[0]:ix[1]].T\n",
    "    h_t[h_t>=-2.5]     = 0     # land points  \n",
    "    #lon = lon[ix[0]:ix[1],jy[0]:jy[1]]\n",
    "    #lat = lat[ix[0]:ix[1],jy[0]:jy[1]]\n",
    "    #Nxh, Nyh = lon_g.shape\n",
    "    dx_h = 1/nc.variables['pm'][:].T\n",
    "    dy_h = 1/nc.variables['pn'][:].T\n",
    "nc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total domain : lon in [-41.8,-23.2], lat in [30.5,44.2]\n",
      " Processor 000 will do lon in [-40.4,-39.4], lat in [30.5,31.4], [13 x 13] points\n"
     ]
    }
   ],
   "source": [
    "# --- grids (coordinates relative to proc number) ---\n",
    "# working with (x,y)-ordered grids\n",
    "if colloc:\n",
    "    lon2d_g = lon_h[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep]\n",
    "    lat2d_g = lat_h[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep]\n",
    "    nlon_g, nlat_g = lon2d_g.shape\n",
    "else:\n",
    "    lon1d_g       = np.arange(lonmin_g,lonmax_g+dspace,dspace) \n",
    "    lat1d_g       = np.arange(latmin_g,latmax_g+dspace,dspace) \n",
    "    nlat_g,nlon_g = lat1d_g.shape[0],lon1d_g.shape[0] \n",
    "di            = nlon_g//npx\n",
    "dj            = nlat_g//npy\n",
    "if di*npx<nlon_g: di+=1 # correction to make sure all the area is covered\n",
    "if dj*npy<nlat_g: dj+=1\n",
    "imin          = di*(rank%npx)\n",
    "jmin          = dj*(rank//npx)\n",
    "imax          = imin+di\n",
    "jmax          = jmin+dj\n",
    "\n",
    "# --- define subgrids ---\n",
    "if colloc:\n",
    "    lon2d, lat2d = lon2d_g[imin:imax,jmin:jmax], lat2d_g[imin:imax,jmin:jmax]\n",
    "else:\n",
    "    lon1d = lon1d_g[imin:imax] \n",
    "    lat1d = lat1d_g[jmin:jmax] \n",
    "    lon2d,lat2d = np.meshgrid(lon1d,lat1d)  \n",
    "nlon,nlat = lon2d.shape\n",
    "lonmin,lonmax = np.nanmin(lon2d),np.nanmax(lon2d)\n",
    "latmin,latmax = np.nanmin(lat2d),np.nanmax(lat2d)\n",
    "\n",
    "if size > 1:\n",
    "    lonall = comm.gather([lonmin,lonmax],root=0)\n",
    "    latall = comm.gather([latmin,latmax],root=0)\n",
    "    dimall = comm.gather([nlat,nlon],root=0)\n",
    "else:\n",
    "    lonall = [[lonmin,lonmax]]\n",
    "    latall = [[latmin,latmax]]\n",
    "    dimall = [[nlat,nlon]]\n",
    "    \n",
    "if rank==0:\n",
    "    print('Total domain : lon in [%.1f,%.1f], lat in [%.1f,%.1f]'\\\n",
    "          %(lon2d_g.min(),lon2d_g.max(),lat2d_g.min(),lat2d_g.max()))\n",
    "    for i in range(size):  \n",
    "        print(' Processor %.3i will do lon in [%.1f,%.1f], lat in [%.1f,%.1f], [%i x %i] points'\\\n",
    "              %(i,lonall[i][0],lonall[i][1],latall[i][0],latall[i][1],dimall[i][1],dimall[i][0]) )\n",
    "\n",
    "# load grid angle for rotation to ellipse frame since we do not interpolate field on regular grid here        \n",
    "if colloc:\n",
    "    nc = Dataset(file_topo,'r')\n",
    "    hgrid = -nc.variables[varh][jy[0]:jy[1]:nstep,ix[0]:ix[1]:nstep].T[imin:imax,jmin:jmax]\n",
    "    hgrid[hgrid>=-2.5] = 0.\n",
    "    angrid = nc.variables['angle'][jy[0]:jy[1]:nstep,ix[0]:ix[1]:nstep].T[imin:imax,jmin:jmax]\n",
    "    nc.close()\n",
    "else:\n",
    "    raise ValueError('colloc False not implementend for hgrid: need to interpolate')\n",
    "    \n",
    "# --- Coriolis frequency [rad s-1] ---\n",
    "f = 2*omega*np.sin(lat2d*np.pi/180.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 0.07158391,  0.07158424,  0.07158459,  0.07158493,  0.07158527,\n",
       "         0.07158562,  0.07158596,  0.07158631,  0.07158666,  0.07158701,\n",
       "         0.07158737,  0.07158772,  0.07158807],\n",
       "       [ 0.07094008,  0.07094041,  0.07094074,  0.07094108,  0.07094141,\n",
       "         0.07094175,  0.07094209,  0.07094242,  0.07094277,  0.07094311,\n",
       "         0.07094345,  0.07094379,  0.07094414],\n",
       "       [ 0.0702247 ,  0.07022502,  0.07022534,  0.07022566,  0.07022599,\n",
       "         0.07022631,  0.07022664,  0.07022697,  0.0702273 ,  0.07022763,\n",
       "         0.07022797,  0.0702283 ,  0.07022864],\n",
       "       [ 0.06950928,  0.0695096 ,  0.06950991,  0.06951022,  0.06951054,\n",
       "         0.06951085,  0.06951117,  0.06951149,  0.06951181,  0.06951213,\n",
       "         0.06951246,  0.06951278,  0.06951311],\n",
       "       [ 0.06879385,  0.06879415,  0.06879445,  0.06879476,  0.06879506,\n",
       "         0.06879537,  0.06879568,  0.06879599,  0.0687963 ,  0.06879661,\n",
       "         0.06879692,  0.06879724,  0.06879755],\n",
       "       [ 0.06807838,  0.06807868,  0.06807897,  0.06807927,  0.06807956,\n",
       "         0.06807986,  0.06808016,  0.06808046,  0.06808076,  0.06808106,\n",
       "         0.06808137,  0.06808167,  0.06808198],\n",
       "       [ 0.0673629 ,  0.06736318,  0.06736347,  0.06736375,  0.06736404,\n",
       "         0.06736433,  0.06736462,  0.06736491,  0.0673652 ,  0.06736549,\n",
       "         0.06736579,  0.06736608,  0.06736638],\n",
       "       [ 0.06664738,  0.06664766,  0.06664793,  0.06664821,  0.06664849,\n",
       "         0.06664877,  0.06664905,  0.06664933,  0.06664961,  0.0666499 ,\n",
       "         0.06665018,  0.06665047,  0.06665076],\n",
       "       [ 0.06593185,  0.06593211,  0.06593238,  0.06593265,  0.06593292,\n",
       "         0.06593319,  0.06593346,  0.06593373,  0.06593401,  0.06593428,\n",
       "         0.06593456,  0.06593484,  0.06593511],\n",
       "       [ 0.06521628,  0.06521654,  0.0652168 ,  0.06521706,  0.06521732,\n",
       "         0.06521758,  0.06521785,  0.06521811,  0.06521838,  0.06521864,\n",
       "         0.06521891,  0.06521918,  0.06521945],\n",
       "       [ 0.0645007 ,  0.06450095,  0.0645012 ,  0.06450145,  0.0645017 ,\n",
       "         0.06450196,  0.06450221,  0.06450247,  0.06450272,  0.06450298,\n",
       "         0.06450324,  0.0645035 ,  0.06450376],\n",
       "       [ 0.06378509,  0.06378533,  0.06378557,  0.06378582,  0.06378606,\n",
       "         0.0637863 ,  0.06378655,  0.0637868 ,  0.06378705,  0.0637873 ,\n",
       "         0.06378755,  0.0637878 ,  0.06378805],\n",
       "       [ 0.06306946,  0.06306969,  0.06306992,  0.06307016,  0.06307039,\n",
       "         0.06307063,  0.06307087,  0.06307111,  0.06307135,  0.06307159,\n",
       "         0.06307183,  0.06307208,  0.06307232]])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "angrid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ extract Tides ------------------------------\n",
    "\n",
    "if tide=='lucky' and collot and colloc:\n",
    "    nc   = Dataset(uname,'r')\n",
    "    phi = nc.variables['tide_Cangle'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "    pha = nc.variables['tide_Cphase'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "    ue = nc.variables['tide_Cmax'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "    ve = nc.variables['tide_Cmin'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "    phi -= angrid    # TODO check this\n",
    "    nc.close()\n",
    "elif tide=='tpxo8':\n",
    "    ue, ve, phi, pha = get_tpxo_on_grid([uname,hname],lon2d,lat2d,ellipse=True,grang=angrid)\n",
    "\n",
    "phi = phi*np.pi/180  # angle between major axis and East  [rad] (beware sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from convert_TPXO_to_ellipses import ellipse as ua2ellipse\n",
    "\n",
    "def rot_uv(uu,vv,ang):\n",
    "    \"\"\" rotate velocity field (uu,vv) by angle \"ang\" (radians) \"\"\"\n",
    "    return uu*np.cos(ang) - vv*np.sin(ang), uu*np.sin(ang) + vv*np.cos(ang)\n",
    "\n",
    "# convert complex to amplitude+phase\n",
    "def cmp2ap(re,im):\n",
    "    return np.abs(re+1j*im),np.arctan2(-im,re)*180/np.pi\n",
    "\n",
    "def get_tpxo_on_grid(filenames,lonr,latr,ellipse=False,grang=None):\n",
    "    \"\"\" read TPXO8 files (filenames=[ufile,hfile]) and interpolate it on lonr, latr grid\n",
    "    if ellipse = True: return ellipse components, otherwise return amplitude, phase for u, v\n",
    "    if grang != None, rotate field by angle grang \"\"\"\n",
    "    uname, hname = filenames\n",
    "\n",
    "    nc = Dataset(uname,'r')\n",
    "    latu = nc.variables['lat_u'][:]\n",
    "    lonu = nc.variables['lon_u'][:]\n",
    "    latv = nc.variables['lat_v'][:]\n",
    "    lonv = nc.variables['lon_v'][:]\n",
    "    nc.close()\n",
    "    #lonu[lonu>180] -= 360\n",
    "    #lonv[lonv>180] -= 360\n",
    "    indxu, = np.where( (lonu>=(lonr%360).min()) & (lonu<=(lonr%360).max()) )\n",
    "    indxv, = np.where( (lonv>=(lonr%360).min()) & (lonv<=(lonr%360).max()) )\n",
    "    indyu, = np.where( (latu>=latr.min()) & (latu<=latr.max()) )\n",
    "    indyv, = np.where( (latv>=latr.min()) & (latv<=latr.max()) )\n",
    "    lonu = lonu[indxu]\n",
    "    latu = latu[indyu]\n",
    "    lonv = lonv[indxv]\n",
    "    latv = latv[indyv]\n",
    "\n",
    "    nc = Dataset(hname,'r')\n",
    "    hu = nc.variables['hu'][indxu,indyu]\n",
    "    hv = nc.variables['hv'][indxv,indyv]\n",
    "    nc.close()\n",
    "\n",
    "    nc = Dataset(uname,'r')\n",
    "    ure = nc.variables['uRe'][indxu,indyu]*1e-4/hu    # cm²/s to m/s\n",
    "    vre = nc.variables['vRe'][indxv,indyv]*1e-4/hv\n",
    "    uim = nc.variables['uIm'][indxu,indyu]*1e-4/hu\n",
    "    vim = nc.variables['vIm'][indxv,indyv]*1e-4/hv\n",
    "    nc.close()\n",
    "    ure[np.isnan(ure)] = 0.\n",
    "    vre[np.isnan(vre)] = 0.\n",
    "    uim[np.isnan(uim)] = 0.\n",
    "    vim[np.isnan(vim)] = 0.\n",
    "\n",
    "    ure = interp.RectBivariateSpline(lonu, latu, ure).ev(lonr%360,latr) # z.shape = (x.size, y.size)\n",
    "    vre = interp.RectBivariateSpline(lonv, latv, vre).ev(lonr%360,latr)\n",
    "    uim = interp.RectBivariateSpline(lonu, latu, uim).ev(lonr%360,latr)\n",
    "    vim = interp.RectBivariateSpline(lonv, latv, vim).ev(lonr%360,latr)\n",
    "\n",
    "    if ellipse:\n",
    "        ua, up = cmp2ap(ure,uim)\n",
    "        va, vp = cmp2ap(vre,vim)\n",
    "        sema, ecc, inc, pha = ua2ellipse(ua,up,va,vp)    # that is really sema, ecc, inc, pha\n",
    "        if grang is not None:\n",
    "            va -= grang\n",
    "        return sema, ecc, inc, pha\n",
    "    else:\n",
    "        if grang is not None:\n",
    "            ure, vre = rot2d(ure,vre,grang)\n",
    "            uim, vim = rot2d(uim, vim, grang)\n",
    "        ua, up = cmp2ap(ure,uim)\n",
    "        va, vp = cmp2ap(vre,vim)\n",
    "        return ua, up, va, vp\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponential interpolation for stratification: N0=0.003103240019632272, b=1358.435795951829\n"
     ]
    }
   ],
   "source": [
    "# ------ extract density profile, compute N2 ------------------\n",
    "if clim == \"lucky\":\n",
    "    nc = Dataset(cname,'r')\n",
    "    T = nc.variables['temp_roms_avg'][:]\n",
    "    S = nc.variables['salt_roms_avg'][:]\n",
    "    zz = nc.variables['depth'][:]\n",
    "    nz = zz.size\n",
    "    \n",
    "rho = np.sort(rhop(T,S)) #SW_Density(T,S) # sorting is cheating here\n",
    "rho0 = rho.mean()\n",
    "frho = interp.pchip(zz[::-1],rho[::-1],extrapolate=True)\n",
    "N2_tmp = -(g/rho0)*(2*np.pi)**2*frho.derivative()(zz)    # # has to be in [(rad s-1)^2]\n",
    "# temporary fixing:\n",
    "if N2_tmp[-1]==0: N2_tmp[-1] = 1e-8\n",
    "indneg, = np.where(N2_tmp<=0.)\n",
    "for ii in indneg:\n",
    "    N2_tmp[ii] = (N2_tmp[ii-1] + N2_tmp[ii+1])/2\n",
    "fN2 = interp.pchip(zz[::-1],N2_tmp,extrapolate=True)    \n",
    "\n",
    "\n",
    "# fit exponential profile\n",
    "slope,intercept,r_val,p_val,std_err = stats.linregress(zz,np.log(N2_tmp**0.5))\n",
    "N0  = np.exp(intercept)/(2*np.pi)\n",
    "b   = 1./slope\n",
    "\n",
    "if doverb:\n",
    "    if indneg.size>0:\n",
    "        print('had to resort stratif for {} values'.format(indneg.size))\n",
    "    print('exponential interpolation for stratification: N0={0}, b={1}'.format(N0,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ------ prepare netcdf file and store \"fix\" variables---------------------------\n",
    "\n",
    "ncw = Dataset(path_write+file_write,'w')\n",
    "\n",
    "ncw.createDimension('z',zz.size)\n",
    "ncw.createDimension('lon',nlon)\n",
    "ncw.createDimension('lat',nlat)\n",
    "ncw.createDimension('kh',nxout)\n",
    "ncw.createDimension('theta',nxoth)\n",
    "#ncw.createDimension('nmodes',nmodes)\n",
    "ncw.createVariable('z','f',('z',))\n",
    "ncw.createVariable('N2z','f',('z'))\n",
    "ncw.createVariable('lon','f',('lat','lon'))\n",
    "ncw.createVariable('lat','f',('lat','lon'))\n",
    "ncw.createVariable('h','f',('lat','lon'))\n",
    "ncw.createVariable('ue','f',('lat','lon'))\n",
    "ncw.createVariable('ve','f',('lat','lon'))\n",
    "var = ncw.createVariable('phi','f',('lat','lon'))\n",
    "var.long_name = 'angle between ellipse major-axis and x-axis'\n",
    "ncw.createVariable('N2b','f',('lat','lon'))\n",
    "ncw.createVariable('N0','f',()) # works even if N0 and b are constant \n",
    "ncw.createVariable('b','f',())\n",
    "ncw.createVariable('f','f',('lat','lon'))\n",
    "ncw.createVariable('kh','f',('kh'))\n",
    "var.long_name = 'equivalent mode number'\n",
    "ncw.createVariable('theta','f',('theta'))\n",
    "var = ncw.createVariable('Ef','f',('lat','lon','kh','theta')) # case 1 \n",
    "var.long_name = 'Energy flux (lat,lon,K,theta)'                 # case 1 \n",
    "var = ncw.createVariable('Ef_a','f',('lat','lon','kh'))         # case 2 \n",
    "var.long_name = 'Azimuthally-averaged energy flux (lat,lon,K)'   # case 2 \n",
    "var = ncw.createVariable('Ef_t','f',('lat','lon'))\n",
    "var.long_name = 'Total energy flux (lat,lon)'\n",
    "var = ncw.createVariable('h_sp','f',('lat','lon','kh','theta'))\n",
    "var.long_name = 'Local spectrum of topography (lat,lon,K,theta)'\n",
    "ncw.variables['z'][:]      = zz\n",
    "ncw.variables['N2z'][:]    = N2_tmp\n",
    "ncw.variables['lon'][:]    = lon2d.T\n",
    "ncw.variables['lat'][:]    = lat2d.T\n",
    "ncw.variables['h'][:]      = hgrid.T\n",
    "ncw.variables['ue'][:]     = ue.T\n",
    "ncw.variables['ve'][:]     = ve.T\n",
    "ncw.variables['phi'][:]    = phi.T\n",
    "ncw.variables['N0'][:]     = N0\n",
    "ncw.variables['b'][:]      = b\n",
    "ncw.variables['f'][:]      = f.T\n",
    "ncw.variables['kh'][:]     = khout\n",
    "ncw.variables['theta'][:]  = thout \n",
    "\n",
    "ncvar = ncw.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> proc 000, time spent : 00 h 56 min 42 sec, computation is at 76.9 percent\n",
      " ===> proc 000, time spent : 00 h 56 min 46 sec, save in netcdf file \n"
     ]
    }
   ],
   "source": [
    "# ====== BEGINNING OF REGIONAL LOOP ON LAT,LON ======================\n",
    "\n",
    "for j in [10]: #range(nlat): #\n",
    "    clock_diff = datetime.now() - clock\n",
    "    hour,sec = divmod(clock_diff.seconds,3600)\n",
    "    hour     = hour + clock_diff.days*24\n",
    "    minu,sec = divmod(sec,60)\n",
    "    print( ' ---> proc %.3i, time spent : %.2i h %.2i min %.2i sec, computation is at %.1f percent'\\\n",
    "          %(rank,hour,minu,sec,float(j)/nlat*100.)) \n",
    "    for i in [10]: #range(nlon): #\n",
    "        xpos, ypos = lon2d[i,j], lat2d[i,j]\n",
    "        ix, jy = np.unravel_index( ((lon_h-xpos)**2 + (lat_h-ypos)**2).argmin() , (nlon_h,nlat_h))\n",
    "        xx = np.cumsum(dx_h[:,jy]) - dx_h[:ix+1,jy].sum()    # substracte midle point\n",
    "        yy = np.cumsum(dy_h[ix,:]) - dy_h[ix,:jy+1].sum()\n",
    "        #degy = Lchk/Erad*180./np.pi\n",
    "        #degx = degy/np.cos(ypos*np.pi/180.)\n",
    "        # endpoints\n",
    "        #il = np.abs(lon_h[:,jy]-xpos+degx).argmin()-2\n",
    "        #ir = np.abs(lon_h[:,jy]-xpos-degx).argmin()+2\n",
    "        #jb = np.abs(lat_h[ix,:]-ypos+degy).argmin()-2\n",
    "        #jt = np.abs(lat_h[ix,:]-ypos-degy).argmin()+2\n",
    "        il = np.abs(xx+Lchk).argmin() - 2    # indices in global (topo) full-resolution grid\n",
    "        ir = np.abs(xx-Lchk).argmin() + 2\n",
    "        jb = np.abs(yy+Lchk).argmin() - 2\n",
    "        jt = np.abs(yy-Lchk).argmin() + 2\n",
    "        \n",
    "        indx = slice(max(il,0),min(ir,nlon_h))\n",
    "        indy = slice(max(jb,0),min(jt,nlat_h))\n",
    "        nc = Dataset(file_topo,'r')\n",
    "        h = h_t[indy,indx].T   \n",
    "        lon = lon_h[indx,indy]\n",
    "        lat = lat_h[indx,indy]\n",
    "        xx, yy = xx[indx], yy[indy]\n",
    "\n",
    "        # approximation: take mean cell size to compute wavenumbers\n",
    "        dxi = 0.5*(dx_h[indy,indx].mean() + dy_h[indy,indx].mean())\n",
    "        nc.close()\n",
    "        #dxi = 0.5*(np.diff(lon,axis=0).mean()*np.cos(ypos*np.pi/180.) \\\n",
    "        #            + np.diff(lat,axis=1).mean())*Erad*np.pi/180.\n",
    "        \n",
    "        # do we need to pad ?\n",
    "        npadl = npadr = npadt = npadb = 0\n",
    "        if -Lchk < xx.min():\n",
    "            if il > 0: print('problem')\n",
    "            npadl = int((xx.min() + Lchk)/dxi) + 1\n",
    "        if xx.max() < Lchk:\n",
    "            if ir < nlon_h: print('problem')\n",
    "            npadr = int((Lchk - xx.max())/dxi) + 1\n",
    "        if -Lchk < yy.min():\n",
    "            if jb > 0: print('problem')\n",
    "            npadb = int((yy.min() + Lchk)/dxi) + 1\n",
    "        if yy.max() < Lchk:\n",
    "            if jt < nlat_h: print('problem')\n",
    "            npadt = int((Lchk - yy.max())/dxi) + 1\n",
    "\n",
    "        if max(npadl,npadr,npadt,npadb) > 0:\n",
    "            h = np.pad(h,((npadl,npadr),(npadb,npadt)),'edge')\n",
    "        # make it square (again)\n",
    "        dn = h.shape[1] - h.shape[0]\n",
    "        if dn < 0:\n",
    "            h = h[(-dn)//2:-((-dn)//2+(-dn)%2),:]\n",
    "        elif dn > 0:\n",
    "            h = h[:,dn//2:-(dn//2+dn%2)]\n",
    "        if np.abs(dn) > 3: print('squaring {0} at i={1}, j={2}'.format(np.abs(dn),i,j))\n",
    "        nx, ny = h.shape\n",
    "        if nx != ny: print('problem still not square')\n",
    "            \n",
    "        kx = np.fft.fftshift(np.fft.fftfreq(nx,dxi))*2*np.pi # wavenumbers in x-direction = major axis\n",
    "        dk = kx[1]-kx[0]\n",
    "        h = detrend_2d(h) # apply bilinear detrend\n",
    "        win_x   = np.tile(np.hanning(nx),(1,1))  # window before filtering \n",
    "        win     = np.dot(win_x.T,win_x)\n",
    "        int_rec = nx**2             # integral of a squared rectangular window (as if no windowing) \n",
    "        int_win = np.nansum(win**2) # integral of the squared window \n",
    "        norm    = (int_rec/int_win)*1/(nx**2*dk**2) # [1/(rad m-1)^2] normalization constant \n",
    "        sp = norm*abs(np.fft.fftshift(np.fft.fft2(h*win)))**2\n",
    "        sp = sp*np.nanvar(h)/np.sum(sp*dk*dk)\n",
    "        kx2d,ky2d = np.meshgrid(kx,kx)\n",
    "        sp[np.where(np.logical_and(kx2d==0,ky2d==0))] = np.nan # remove continuous component\n",
    "        \n",
    "        sp_polar, r, theta = reproject_image_into_polar(sp.T,origin=(nx//2,nx//2),theta_shift=-(phi[i,j]-angrid[i,j]))\n",
    "        kh = r*dk # r is in pixel, multiply by dk to get wavenumber\n",
    "        sp_polar[r==0]=np.nan\n",
    "\n",
    "            \n",
    "        weight = ( ue[i,j]**2*np.cos(theta)**2 + ve[i,j]**2*np.sin(theta)**2 )\n",
    "        gamma = sp_polar*weight[None,:]*kh[:,None]\n",
    "            \n",
    "        N2b = fN2(hgrid[i,j])\n",
    "        \n",
    "        # --- compute Ef(K,theta) ---\n",
    "        coef = 0.5*rho0*((N2b-M2**2)*(M2**2-f[i,j]**2))**0.5/M2 \n",
    "        Ef = coef*gamma \n",
    "\n",
    "        # --- azimuthal integration [0,2pi] ---\n",
    "        dtheta = theta[1] - theta[0] \n",
    "        Ef_a = np.nansum(Ef*dtheta,axis=1)*kh/(2*np.pi)\n",
    "\n",
    "        # --- equivalent mode number Eq (6) in StL and G 2002 ---  \n",
    "        k1 = np.pi*(M2**2-f[i,j]**2)**0.5/(b*N0) # Eq (6), H neglected\n",
    "        dkj = k1 \n",
    "        \n",
    "        try:   \n",
    "            kmin_int = np.nanargmin(abs(kh-(k1-0.5*dkj)))+1 # min index over which performing integral\n",
    "        except: # land points\n",
    "            kmin_int = -1\n",
    "        \n",
    "        # store in netCDF file NRJ fluxes\n",
    "        ncvar['Ef'][j,i,:,:] = interp.RectBivariateSpline(kh,theta,Ef,kx=1,ky=1)(khout,thout)  # energy flux sp. density\n",
    "        ncvar['Ef_a'][j,i,:] = interp.pchip(kh,Ef_a)(khout)     # NRJ flux azimuth.-averaged\n",
    "        ncvar['Ef_t'][j,i] = np.nansum(Ef_a[kmin_int:]*dk) # total NRJ flux\n",
    "        ncvar['h_sp'][j,i,:,:] = interp.RectBivariateSpline(kh,theta,sp_polar,kx=1,ky=1)(khout,thout) # spectrum of topography\n",
    "        ncvar['N2b'][j,i]    = N2b\n",
    "# ------ end of loop: print timing ---------------------------\n",
    "ncw.close()\n",
    "\n",
    "clock_diff = datetime.now() - clock\n",
    "hour,sec = divmod(clock_diff.seconds,3600)\n",
    "hour     = hour + clock_diff.days*24\n",
    "minu,sec = divmod(sec,60)\n",
    "print(' ===> proc %.3i, time spent : %.2i h %.2i min %.2i sec, save in netcdf file '\\\n",
    "      %(rank,hour,minu,sec)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1429.0631359956851"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "h.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
