{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib     \n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams['font.family'] = 'sans-serif'\n",
    "plt.rcParams['font.sans-serif'] = 'Arial'\n",
    "plt.rcParams['text.usetex'] = False\n",
    "import scipy.interpolate as itp\n",
    "import scipy.signal as sig\n",
    "import scipy.stats as stats\n",
    "from netCDF4 import Dataset\n",
    "from SW_Density import SW_Density as rhop # temporary\n",
    "#from comp_rho import rhop\n",
    "from distance_sphere_matproof import dist_sphere_matproof\n",
    "from convert_TPXO_to_ellipses import ellipse\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from datetime import datetime\n",
    "from change_coord import reproject_image_into_polar\n",
    "#from mpi4py import MPI \n",
    "from pad_coords import pad_coords   # for padding fields outside of domain\n",
    "from detrend_2d import detrend_2d\n",
    "clock  = datetime.now()\n",
    "\n",
    "doverb = True\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mpi size is 1 vs. 4x4\n"
     ]
    }
   ],
   "source": [
    "# --- MPI parameters --- \n",
    "npx,npy = 4,4 # number of processors in x and y directions  \n",
    "#npx,npy = 1,1 # number of processors in x and y directions  \n",
    "#comm = MPI.COMM_WORLD\n",
    "#rank = comm.Get_rank()\n",
    "#size = comm.Get_size()\n",
    "rank, size = 0, 1\n",
    "if size!=npx*npy:\n",
    "    print('mpi size is {0} vs. {1}x{2}'.format(size,npx,npy))\n",
    "    #raise ValueError('number of subdomains different from number of procs -> failure!')\n",
    "\n",
    "# --- data location ---\n",
    "path_data = './input_data/' #'/data0/project/vortex/lahaye/Tide_Conv/input_data/' #\n",
    "#path_data = '/net/krypton'+path_data     # if not on LOPS hub\n",
    "\n",
    "# --- climato ---\n",
    "clim = \"lucky\"\n",
    "if clim == \"lucky\":\n",
    "    cname = path_data+\"lucky_ts_meanSODA_winter.nc\"\n",
    "\n",
    "# --- topography dataset --- \n",
    "topo = \"lucky\"\n",
    "\n",
    "# --- tide dataset --- \n",
    "collot = True    # tides and topo defined on same points // False not implemented\n",
    "tide = 'lucky'\n",
    "uname     = path_data+'luckym2_frc.nc' # see read_write_TPXO8_netcdf.py\n",
    "\n",
    "# --- global grid ---   \n",
    "colloc = True    # use same grid as topo // False not implemented\n",
    "if colloc:\n",
    "    nstep = 10\n",
    "else:\n",
    "    dspace            = 1e3       # resolution of the final map in meter\n",
    "#lonmin_g,lonmax_g = -33,-32     # test box // None takes the whole domain defined by h  \n",
    "#latmin_g,latmax_g = 37,37.7  \n",
    "lonmin_g, latmin_g, lonmax_g, latmax_g = [None]*4  # entire domain\n",
    "\n",
    "# --- output location and name --- \n",
    "path_write = path_data.replace('input','output')\n",
    "file_write = 'Ef_'+topo+'_mpi%.3i.nc'%rank  # lonmin,lonmax,latmin,latmax \n",
    "#file_write = 'test_npts400_%.2i.nc'%rank  # lonmin,lonmax,latmin,latmax \n",
    "\n",
    "# --- miscellaneous --- \n",
    "if topo == \"lucky\":\n",
    "    file_topo = path_data+\"lucky_grd.nc\"\n",
    "    Lchk = 150e3    # length of window for computing topo spectrum\n",
    "    varx, vary = 'lon_rho', 'lat_rho'\n",
    "    varh = 'h'\n",
    "\n",
    "zmin       = -100              # min depth to compute Ef [m], below contin. shelf roughly\n",
    "g          = 9.81              # gravity [m s-2]\n",
    "omega      = 7.2921e-5         # Earth's rotation rate [rad s-1]\n",
    "M2         = 2.*np.pi/(44700.) # M2 tide frequency [rad s-1] \n",
    "lonm,latm  = -32.16,37.17      # Momar mooring location \n",
    "Erad = 6371e3                  # Earth radius [m]\n",
    "\n",
    "### warning: make sure the foillowing lines are in agreement with subsequent parameters and grids\n",
    "nxout = 256\n",
    "nxoth = 128\n",
    "khout = np.linspace(1./Lchk,1./750/np.sqrt(2.),nxout)*np.pi  # k adim for output = k*bar(N)*H/sqrt(M2^2-f^2)\n",
    "thout = np.linspace(0,np.pi*2,nxoth+1)[:-1]  # theta for output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------ get mean topo on the grid, will be used to get N2b --------\n",
    "# load the entire grid (regional modelling purpose)\n",
    "nc   = Dataset(file_topo,'r') # etopo2 and srtm30 files have the same structure\n",
    "if topo == 'lucky':\n",
    "    lon_h = nc.variables[varx][:].T\n",
    "    lat_h = nc.variables[vary][:].T\n",
    "    nlon_h, nlat_h = lon_h.shape\n",
    "    if lonmin_g is None:\n",
    "        ix = [0]\n",
    "    else:\n",
    "        ix = [np.abs(lon_h-lonmin_g).argmin(axis=0).min()]\n",
    "    if lonmax_g is None:\n",
    "        ix.append(nlon_h)\n",
    "    else:\n",
    "        ix.append(np.abs(lon_h-lonmax_g).argmin(axis=0).max())\n",
    "    if latmin_g is None:\n",
    "        jy = [0]\n",
    "    else:\n",
    "        jy = [np.abs(lat_h-latmin_g).argmin(axis=1).min()]\n",
    "    if latmax_g is None:\n",
    "        jy.append(nlat_h)\n",
    "    else:\n",
    "        jy.append(np.abs(lat_h-latmax_g).argmin(axis=1).max())\n",
    "    #h = -nc.variables[varh][jy[0]:jy[1],ix[0]:ix[1]].T\n",
    "    #hgrid[hgrid>=-2.5]     = 0     # land points  \n",
    "    #lon = lon[ix[0]:ix[1],jy[0]:jy[1]]\n",
    "    #lat = lat[ix[0]:ix[1],jy[0]:jy[1]]\n",
    "    #Nxh, Nyh = lon_g.shape\n",
    "    dx_h = 1/nc.variables['pm'][:].T\n",
    "    dy_h = 1/nc.variables['pn'][:].T\n",
    "nc.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total domain : lon in [-41.8,-23.2], lat in [30.5,44.2]\n",
      " Processor 000 will do lon in [-40.6,-36.4], lat in [30.5,34.1], [51 x 51] points\n"
     ]
    }
   ],
   "source": [
    "# --- grids (coordinates relative to proc number) ---\n",
    "# working with (x,y)-ordered grids\n",
    "if colloc:\n",
    "    lon2d_g = lon_h[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep]\n",
    "    lat2d_g = lat_h[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep]\n",
    "    nlon_g, nlat_g = lon2d_g.shape\n",
    "else:\n",
    "    lon1d_g       = np.arange(lonmin_g,lonmax_g+dspace,dspace) \n",
    "    lat1d_g       = np.arange(latmin_g,latmax_g+dspace,dspace) \n",
    "    nlat_g,nlon_g = lat1d_g.shape[0],lon1d_g.shape[0] \n",
    "di            = nlon_g//npx\n",
    "dj            = nlat_g//npy\n",
    "if di*npx<nlon_g: di+=1 # correction to make sure all the area is covered\n",
    "if dj*npy<nlat_g: dj+=1\n",
    "imin          = di*(rank%npx)\n",
    "jmin          = dj*(rank//npx)\n",
    "imax          = imin+di\n",
    "jmax          = jmin+dj\n",
    "\n",
    "# --- define subgrids ---\n",
    "if colloc:\n",
    "    lon2d, lat2d = lon2d_g[imin:imax,jmin:jmax], lat2d_g[imin:imax,jmin:jmax]\n",
    "else:\n",
    "    lon1d = lon1d_g[imin:imax] \n",
    "    lat1d = lat1d_g[jmin:jmax] \n",
    "    lon2d,lat2d = np.meshgrid(lon1d,lat1d)  \n",
    "nlon,nlat = lon2d.shape\n",
    "lonmin,lonmax = np.nanmin(lon2d),np.nanmax(lon2d)\n",
    "latmin,latmax = np.nanmin(lat2d),np.nanmax(lat2d)\n",
    "\n",
    "if size > 1:\n",
    "    lonall = comm.gather([lonmin,lonmax],root=0)\n",
    "    latall = comm.gather([latmin,latmax],root=0)\n",
    "    dimall = comm.gather([nlat,nlon],root=0)\n",
    "else:\n",
    "    lonall = [[lonmin,lonmax]]\n",
    "    latall = [[latmin,latmax]]\n",
    "    dimall = [[nlat,nlon]]\n",
    "    \n",
    "if rank==0:\n",
    "    print('Total domain : lon in [%.1f,%.1f], lat in [%.1f,%.1f]'\\\n",
    "          %(lon2d_g.min(),lon2d_g.max(),lat2d_g.min(),lat2d_g.max()))\n",
    "    for i in range(size):  \n",
    "        print(' Processor %.3i will do lon in [%.1f,%.1f], lat in [%.1f,%.1f], [%i x %i] points'\\\n",
    "              %(i,lonall[i][0],lonall[i][1],latall[i][0],latall[i][1],dimall[i][1],dimall[i][0]) )\n",
    "\n",
    "# load grid angle for rotation to ellipse frame since we do not interpolate field on regular grid here        \n",
    "if colloc:\n",
    "    nc = Dataset(file_topo,'r')\n",
    "    hgrid = -nc.variables[varh][jy[0]:jy[1]:nstep,ix[0]:ix[1]:nstep].T[imin:imax,jmin:jmax]\n",
    "    hgrid[hgrid>=-2.5] = 0.\n",
    "    angrid = nc.variables['angle'][jy[0]:jy[1]:nstep,ix[0]:ix[1]:nstep].T[imin:imax,jmin:jmax]\n",
    "    nc.close()\n",
    "else:\n",
    "    raise ValueError('colloc False not implementend for hgrid: need to interpolate')\n",
    "    \n",
    "# --- Coriolis frequency [rad s-1] ---\n",
    "f = 2*omega*np.sin(lat2d*np.pi/180.) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------ extract Tides ------------------------------\n",
    "nc   = Dataset(uname,'r') # variables are on C-grid \n",
    "\n",
    "if not (tide=='lucky' and collot and colloc):\n",
    "    raise ValueError('choice for tide and collot not implemented')\n",
    "\n",
    "phi = nc.variables['tide_Cangle'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "pha = nc.variables['tide_Cphase'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "ue = nc.variables['tide_Cmax'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "ve = nc.variables['tide_Cmin'][0,...].T[ix[0]:ix[1]:nstep,jy[0]:jy[1]:nstep][imin:imax,jmin:jmax]\n",
    "\n",
    "nc.close()\n",
    "\n",
    "phi = phi*np.pi/180  # angle between major axis and east [rad] (beware sign)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "exponential interpolation for stratification: N0=0.003103240019632274, b=1358.4357959518281\n"
     ]
    }
   ],
   "source": [
    "# ------ extract density profile, compute N2 ------------------\n",
    "if clim == \"lucky\":\n",
    "    nc = Dataset(cname,'r')\n",
    "    T = nc.variables['temp_roms_avg'][:]\n",
    "    S = nc.variables['salt_roms_avg'][:]\n",
    "    zz = nc.variables['depth'][:]\n",
    "    nz = zz.size\n",
    "    \n",
    "rho = np.sort(rhop(T,S)) #SW_Density(T,S) # sorting is cheating here\n",
    "rho0 = rho.mean()\n",
    "frho = itp.pchip(zz[::-1],rho[::-1],extrapolate=True)\n",
    "N2_tmp = -(g/rho0)*(2*np.pi)**2*frho.derivative()(zz)    # # has to be in [(rad s-1)^2]\n",
    "# temporary fixing:\n",
    "if N2_tmp[-1]==0: N2_tmp[-1] = 1e-8\n",
    "indneg, = np.where(N2_tmp<=0.)\n",
    "for ii in indneg:\n",
    "    N2_tmp[ii] = (N2_tmp[ii-1] + N2_tmp[ii+1])/2\n",
    "fN2 = itp.pchip(zz[::-1],N2_tmp,extrapolate=True)    \n",
    "\n",
    "\n",
    "# fit exponential profile\n",
    "slope,intercept,r_val,p_val,std_err = stats.linregress(zz,np.log(N2_tmp**0.5))\n",
    "N0  = np.exp(intercept)/(2*np.pi)\n",
    "b   = 1./slope\n",
    "\n",
    "if doverb:\n",
    "    if indneg.size>0:\n",
    "        print('had to resort stratif for {} values'.format(indneg.size))\n",
    "    print('exponential interpolation for stratification: N0={0}, b={1}'.format(N0,b))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ------ prepare netcdf file and store \"fix\" variables---------------------------\n",
    "\n",
    "ncw = Dataset(path_write+file_write,'w')\n",
    "\n",
    "ncw.createDimension('z',zz.size)\n",
    "ncw.createDimension('lon',nlon)\n",
    "ncw.createDimension('lat',nlat)\n",
    "ncw.createDimension('kh',nxout)\n",
    "ncw.createDimension('theta',nxoth)\n",
    "#ncw.createDimension('nmodes',nmodes)\n",
    "ncw.createVariable('z','f',('z',))\n",
    "ncw.createVariable('N2z','f',('z'))\n",
    "ncw.createVariable('lon','f',('lat','lon'))\n",
    "ncw.createVariable('lat','f',('lat','lon'))\n",
    "ncw.createVariable('h','f',('lat','lon'))\n",
    "ncw.createVariable('ue','f',('lat','lon'))\n",
    "ncw.createVariable('ve','f',('lat','lon'))\n",
    "var = ncw.createVariable('phi','f',('lat','lon'))\n",
    "var.long_name = 'angle between ellipse major-axis and x-axis'\n",
    "ncw.createVariable('N2b','f',('lat','lon'))\n",
    "ncw.createVariable('N0','f',()) # works even if N0 and b are constant \n",
    "ncw.createVariable('b','f',())\n",
    "ncw.createVariable('f','f',('lat','lon'))\n",
    "ncw.createVariable('kh','f',('kh'))\n",
    "var.long_name = 'equivalent mode number'\n",
    "ncw.createVariable('theta','f',('theta'))\n",
    "var = ncw.createVariable('Ef','f',('lat','lon','kh','theta')) # case 1 \n",
    "var.long_name = 'Energy flux (lat,lon,K,theta)'                 # case 1 \n",
    "var = ncw.createVariable('Ef_a','f',('lat','lon','kh'))         # case 2 \n",
    "var.long_name = 'Azimuthally-averaged energy flux (lat,lon,K)'   # case 2 \n",
    "var = ncw.createVariable('Ef_t','f',('lat','lon'))\n",
    "var.long_name = 'Total energy flux (lat,lon)'\n",
    "var = ncw.createVariable('h_sp','f',('lat','lon','kh','theta'))\n",
    "var.long_name = 'Local spectrum of topography (lat,lon,K,theta)'\n",
    "ncw.variables['z'][:]      = zz\n",
    "ncw.variables['N2z'][:]    = N2_tmp\n",
    "ncw.variables['lon'][:]    = lon2d.T\n",
    "ncw.variables['lat'][:]    = lat2d.T\n",
    "ncw.variables['h'][:]      = hgrid.T\n",
    "ncw.variables['ue'][:]     = ue.T\n",
    "ncw.variables['ve'][:]     = ve.T\n",
    "ncw.variables['phi'][:]    = phi.T\n",
    "ncw.variables['N0'][:]     = N0\n",
    "ncw.variables['b'][:]      = b\n",
    "ncw.variables['f'][:]      = f.T\n",
    "ncw.variables['kh'][:]     = khout\n",
    "ncw.variables['theta'][:]  = thout \n",
    "\n",
    "ncvar = ncw.variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " ---> proc 000, time spent : 00 h 15 min 41 sec, computation is at 0.0 percent\n",
      " ---> proc 000, time spent : 00 h 15 min 52 sec, computation is at 2.0 percent\n",
      " ---> proc 000, time spent : 00 h 16 min 03 sec, computation is at 3.9 percent\n",
      " ---> proc 000, time spent : 00 h 16 min 14 sec, computation is at 5.9 percent\n",
      " ---> proc 000, time spent : 00 h 16 min 25 sec, computation is at 7.8 percent\n",
      " ---> proc 000, time spent : 00 h 16 min 35 sec, computation is at 9.8 percent\n",
      " ---> proc 000, time spent : 00 h 16 min 46 sec, computation is at 11.8 percent\n",
      " ---> proc 000, time spent : 00 h 16 min 56 sec, computation is at 13.7 percent\n",
      " ---> proc 000, time spent : 00 h 17 min 07 sec, computation is at 15.7 percent\n",
      " ---> proc 000, time spent : 00 h 17 min 17 sec, computation is at 17.6 percent\n",
      " ---> proc 000, time spent : 00 h 17 min 27 sec, computation is at 19.6 percent\n",
      " ---> proc 000, time spent : 00 h 17 min 37 sec, computation is at 21.6 percent\n",
      " ---> proc 000, time spent : 00 h 17 min 47 sec, computation is at 23.5 percent\n",
      " ---> proc 000, time spent : 00 h 17 min 57 sec, computation is at 25.5 percent\n",
      " ---> proc 000, time spent : 00 h 18 min 07 sec, computation is at 27.5 percent\n",
      " ---> proc 000, time spent : 00 h 18 min 16 sec, computation is at 29.4 percent\n",
      " ---> proc 000, time spent : 00 h 18 min 26 sec, computation is at 31.4 percent\n",
      " ---> proc 000, time spent : 00 h 18 min 35 sec, computation is at 33.3 percent\n",
      " ---> proc 000, time spent : 00 h 18 min 45 sec, computation is at 35.3 percent\n",
      " ---> proc 000, time spent : 00 h 18 min 54 sec, computation is at 37.3 percent\n",
      " ---> proc 000, time spent : 00 h 19 min 04 sec, computation is at 39.2 percent\n",
      " ---> proc 000, time spent : 00 h 19 min 13 sec, computation is at 41.2 percent\n",
      " ---> proc 000, time spent : 00 h 19 min 24 sec, computation is at 43.1 percent\n",
      " ---> proc 000, time spent : 00 h 19 min 34 sec, computation is at 45.1 percent\n",
      " ---> proc 000, time spent : 00 h 19 min 45 sec, computation is at 47.1 percent\n",
      " ---> proc 000, time spent : 00 h 19 min 55 sec, computation is at 49.0 percent\n",
      " ---> proc 000, time spent : 00 h 20 min 06 sec, computation is at 51.0 percent\n",
      " ---> proc 000, time spent : 00 h 20 min 16 sec, computation is at 52.9 percent\n",
      " ---> proc 000, time spent : 00 h 20 min 27 sec, computation is at 54.9 percent\n",
      " ---> proc 000, time spent : 00 h 20 min 37 sec, computation is at 56.9 percent\n",
      " ---> proc 000, time spent : 00 h 20 min 49 sec, computation is at 58.8 percent\n",
      " ---> proc 000, time spent : 00 h 21 min 01 sec, computation is at 60.8 percent\n",
      " ---> proc 000, time spent : 00 h 21 min 13 sec, computation is at 62.7 percent\n",
      " ---> proc 000, time spent : 00 h 21 min 25 sec, computation is at 64.7 percent\n",
      " ---> proc 000, time spent : 00 h 21 min 36 sec, computation is at 66.7 percent\n",
      " ---> proc 000, time spent : 00 h 21 min 48 sec, computation is at 68.6 percent\n",
      " ---> proc 000, time spent : 00 h 22 min 00 sec, computation is at 70.6 percent\n",
      " ---> proc 000, time spent : 00 h 22 min 11 sec, computation is at 72.5 percent\n",
      " ---> proc 000, time spent : 00 h 22 min 23 sec, computation is at 74.5 percent\n",
      " ---> proc 000, time spent : 00 h 22 min 35 sec, computation is at 76.5 percent\n",
      " ---> proc 000, time spent : 00 h 22 min 46 sec, computation is at 78.4 percent\n",
      " ---> proc 000, time spent : 00 h 22 min 58 sec, computation is at 80.4 percent\n",
      " ---> proc 000, time spent : 00 h 23 min 09 sec, computation is at 82.4 percent\n",
      " ---> proc 000, time spent : 00 h 23 min 20 sec, computation is at 84.3 percent\n",
      " ---> proc 000, time spent : 00 h 23 min 31 sec, computation is at 86.3 percent\n",
      " ---> proc 000, time spent : 00 h 23 min 42 sec, computation is at 88.2 percent\n",
      " ---> proc 000, time spent : 00 h 23 min 52 sec, computation is at 90.2 percent\n",
      " ---> proc 000, time spent : 00 h 24 min 03 sec, computation is at 92.2 percent\n",
      " ---> proc 000, time spent : 00 h 24 min 14 sec, computation is at 94.1 percent\n",
      " ---> proc 000, time spent : 00 h 24 min 25 sec, computation is at 96.1 percent\n",
      " ---> proc 000, time spent : 00 h 24 min 36 sec, computation is at 98.0 percent\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "NetCDF: Not a valid ID",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-31-2528ef5f3391>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m    113\u001b[0m         \u001b[0mncvar\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'N2b'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m    \u001b[0;34m=\u001b[0m \u001b[0mN2b\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    114\u001b[0m \u001b[0;31m# ------ end of loop: print timing ---------------------------\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 115\u001b[0;31m \u001b[0mnc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    116\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[0mclock_diff\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdatetime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mclock\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mnetCDF4/_netCDF4.pyx\u001b[0m in \u001b[0;36mnetCDF4._netCDF4.Dataset.close (netCDF4/_netCDF4.c:16586)\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: NetCDF: Not a valid ID"
     ]
    }
   ],
   "source": [
    "# ====== BEGINNING OF REGIONAL LOOP ON LAT,LON ======================\n",
    "\n",
    "for j in range(nlat): #\n",
    "    clock_diff = datetime.now() - clock\n",
    "    hour,sec = divmod(clock_diff.seconds,3600)\n",
    "    hour     = hour + clock_diff.days*24\n",
    "    minu,sec = divmod(sec,60)\n",
    "    print( ' ---> proc %.3i, time spent : %.2i h %.2i min %.2i sec, computation is at %.1f percent'\\\n",
    "          %(rank,hour,minu,sec,float(j)/nlat*100.)) \n",
    "    for i in range(nlon): #\n",
    "        xpos, ypos = lon2d[i,j], lat2d[i,j]\n",
    "        ix, jy = np.unravel_index( ((lon_h-xpos)**2 + (lat_h-ypos)**2).argmin() , (nlon_h,nlat_h))\n",
    "        xx = np.cumsum(dx_h[:,jy]) - dx_h[:ix+1,jy].sum()\n",
    "        yy = np.cumsum(dy_h[ix,:]) - dy_h[ix,:jy+1].sum()\n",
    "        #degy = Lchk/Erad*180./np.pi\n",
    "        #degx = degy/np.cos(ypos*np.pi/180.)\n",
    "        # endpoints\n",
    "        #il = np.abs(lon_h[:,jy]-xpos+degx).argmin()-2\n",
    "        #ir = np.abs(lon_h[:,jy]-xpos-degx).argmin()+2\n",
    "        #jb = np.abs(lat_h[ix,:]-ypos+degy).argmin()-2\n",
    "        #jt = np.abs(lat_h[ix,:]-ypos-degy).argmin()+2\n",
    "        il = np.abs(xx+Lchk).argmin() - 2\n",
    "        ir = np.abs(xx-Lchk).argmin() + 2\n",
    "        jb = np.abs(yy+Lchk).argmin() - 2\n",
    "        jt = np.abs(yy-Lchk).argmin() + 2\n",
    "        \n",
    "        indx = slice(max(il,0),min(ir,nlon_h))\n",
    "        indy = slice(max(jb,0),min(jt,nlat_h))\n",
    "        nc = Dataset(file_topo,'r')\n",
    "        h = -nc.variables[varh][indy,indx].T   # working with depth\n",
    "        lon = lon_h[indx,indy]\n",
    "        lat = lat_h[indx,indy]\n",
    "        xx, yy = xx[indx], yy[indy]\n",
    "\n",
    "        # approximation: take mean cell size to compute wavenumbers\n",
    "        dxi = 0.5*(dx_h[indy,indx].mean() + dy_h[indy,indx].mean())\n",
    "        nc.close()\n",
    "        #dxi = 0.5*(np.diff(lon,axis=0).mean()*np.cos(ypos*np.pi/180.) \\\n",
    "        #            + np.diff(lat,axis=1).mean())*Erad*np.pi/180.\n",
    "        \n",
    "        # do we need to pad ?\n",
    "        npadl = npadr = npadt = npadb = 0\n",
    "        if -Lchk < xx.min():\n",
    "            if il > 0: print('problem')\n",
    "            npadl = int((xx.min() + Lchk)/dxi) + 1\n",
    "        if xx.max() < Lchk:\n",
    "            if ir < nlon_h: print('problem')\n",
    "            npadr = int((Lchk - xx.max())/dxi) + 1\n",
    "        if -Lchk < yy.min():\n",
    "            if jb > 0: print('problem')\n",
    "            npadb = int((yy.min() + Lchk)/dxi) + 1\n",
    "        if yy.max() < Lchk:\n",
    "            if jt < nlat_h: print('problem')\n",
    "            npadt = int((Lchk - yy.max())/dxi) + 1\n",
    "    \n",
    "\n",
    "        if max(npadl,npadr,npadt,npadb) > 0:\n",
    "            h = np.pad(h,((npadl,npadr),(npadb,npadt)),'edge')\n",
    "        # make it square (again)\n",
    "        dn = h.shape[1] - h.shape[0]\n",
    "        if dn < 0:\n",
    "            h = h[(-dn)//2:-((-dn)//2+(-dn)%2),:]\n",
    "        elif dn > 0:\n",
    "            h = h[:,dn//2:-(dn//2+dn%2)]\n",
    "        if np.abs(dn) > 3: print('squaring {0} at i={1}, j={2}'.format(np.abs(dn),i,j))\n",
    "        nx, ny = h.shape\n",
    "        if nx != ny: print('problem still not square')\n",
    "            \n",
    "        kx = np.fft.fftshift(np.fft.fftfreq(nx,dxi))*2*np.pi # wavenumbers in x-direction = major axis\n",
    "        dk = kx[1]-kx[0]\n",
    "        h = detrend_2d(h) # apply bilinear detrend\n",
    "        win_x   = np.tile(np.hanning(nx),(1,1))  # window before filtering \n",
    "        win     = np.dot(win_x.T,win_x)\n",
    "        int_rec = nx**2             # integral of a squared rectangular window (as if no windowing) \n",
    "        int_win = np.nansum(win**2) # integral of the squared window \n",
    "        norm    = (int_rec/int_win)*1/(nx**2*dk**2) # [1/(rad m-1)^2] normalization constant \n",
    "        sp = norm*abs(np.fft.fftshift(np.fft.fft2(h*win)))**2\n",
    "        sp = sp*np.nanvar(h)/np.sum(sp*dk*dk)\n",
    "        kx2d,ky2d = np.meshgrid(kx,kx)\n",
    "        sp[np.where(np.logical_and(kx2d==0,ky2d==0))] = np.nan # remove continuous component\n",
    "        \n",
    "        sp_polar, r, theta = reproject_image_into_polar(sp.T,origin=(nx//2,nx//2),theta_shift=-(phi[i,j]-angrid[i,j]))\n",
    "        kh = r*dk # r is in pixel, multiply by dk to get wavenumber\n",
    "        sp_polar[sp_polar==0]=np.nan\n",
    "\n",
    "            \n",
    "        weight = ( ue[i,j]**2*np.cos(theta)**2 + ve[i,j]**2*np.sin(theta)**2 )\n",
    "        gamma = sp_polar*weight[None,:]*kh[:,None]\n",
    "            \n",
    "        N2b = fN2(hgrid[i,j])\n",
    "        \n",
    "        # --- compute Ef(K,theta) ---\n",
    "        coef = 0.5*rho0*((N2b-M2**2)*(M2**2-f[i,j]**2))**0.5/M2 \n",
    "        Ef = coef*gamma \n",
    "\n",
    "        # --- azimuthal integration [0,2pi] ---\n",
    "        dtheta = theta[1] - theta[0] \n",
    "        Ef_a = np.nansum(Ef*dtheta,axis=1)*kh/(2*np.pi)\n",
    "\n",
    "        # --- equivalent mode number Eq (6) in StL and G 2002 ---  \n",
    "        k1 = np.pi*(M2**2-f[i,j]**2)**0.5/(b*N0) # Eq (6), H neglected\n",
    "        dkj = k1 \n",
    "        # kmin_int : min index over which performing integral\n",
    "        try:    # ocean points\n",
    "            kmin_int = np.nanargmin(abs(kh-(k1-0.5*dkj)))+1 \n",
    "        except: # land points\n",
    "            kmin_int = -1\n",
    "        \n",
    "        # store in netCDF file NRJ fluxes\n",
    "        ncvar['Ef'][j,i,:,:] = itp.RectBivariateSpline(kh,theta,Ef,kx=1,ky=1)(khout,thout)  # energy flux sp. density\n",
    "        ncvar['Ef_a'][j,i,:] = itp.pchip(kh,Ef_a)(khout)     # NRJ flux azimuth.-averaged\n",
    "        ncvar['Ef_t'][j,i] = np.nansum(Ef_a[kmin_int:]*dk) # total NRJ flux\n",
    "        ncvar['h_sp'][j,i,:,:] = itp.RectBivariateSpline(kh,theta,sp_polar,kx=1,ky=1)(khout,thout) # spectrum of topography\n",
    "        ncvar['N2b'][j,i]    = N2b\n",
    "# ------ end of loop: print timing ---------------------------\n",
    "ncw.close()\n",
    "\n",
    "clock_diff = datetime.now() - clock\n",
    "hour,sec = divmod(clock_diff.seconds,3600)\n",
    "hour     = hour + clock_diff.days*24\n",
    "minu,sec = divmod(sec,60)\n",
    "print(' ===> proc %.3i, time spent : %.2i h %.2i min %.2i sec, save in netcdf file '\\\n",
    "      %(rank,hour,minu,sec)) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.4.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
